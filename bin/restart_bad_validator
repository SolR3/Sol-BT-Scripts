#!/usr/bin/env python3

# standard imports
import argparse
from concurrent.futures import ThreadPoolExecutor
import json
import multiprocessing
import os
import pty
import random
import re
import shlex
import subprocess
import tempfile
import threading
import time


# Constants
DEFAULT_UPDATED_THRESHOLD = 1500
DEFAULT_VTRUST_THRESHOLD = 0.01
DEFAULT_STOPPED_LOGS_THRESHOLD = 30
DEFAULT_LOG_ERRORS_RESTART_WAIT_TIME = 3
TOTAL_CHECKS = 6  # TODO: Calculate this instead of hard-coding it.

# Debugging
DEBUG = False

# Multiprocessing Queues
UPDATED_MP_QUEUE = None
VTRUST_MP_QUEUE = None

def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "-n",
        type=int,
        required=True,
        dest="netuid",
        help="The uid of the subnet.")

    parser.add_argument(
        "--restart-script",
        required=True,
        help="The restart script path.")

    parser.add_argument(
        "--restart-venv",
        help="The restart venv path.")

    parser.add_argument(
        "--pm2-process", # Keeping arg name singular to avoid confusion.
        nargs="+",
        default=[],
        dest="pm2_processes",
        help="Restart the validator based on pm2 log patterns or stopped pm2 "
             "log output. The value passed to this arg is the name of the pm2 "
             "process to monitor. Multiple pm2 processes may be passed to this "
             "arg.")

    parser.add_argument(
        "--docker-container", # Keeping arg name singular to avoid confusion.
        nargs="+",
        default=[],
        dest="docker_containers",
        help="Restart the validator based on docker log patterns. The value "
             "passed to this arg is the name of the docker container to monitor. "
             "Multiple docker containers may be passed to this arg.")

    parser.add_argument(
        "--local-subtensor",
        help="Deprecated. This arg no longer needs to be specified. "
             "The restarter automatically rotates between all local "
             "subtensors.")

    parser.add_argument(
        "--updated-threshold",
        type=int,
        default=DEFAULT_UPDATED_THRESHOLD,
        help="The Updated threshold value above which to restart the "
             "validator. This value is in blocks. "
             f"Default: {DEFAULT_UPDATED_THRESHOLD}")

    parser.add_argument(
        "--vtrust-threshold",
        type=float,
        default=DEFAULT_VTRUST_THRESHOLD,
        help="The vTrust threshold value below which to restart the "
             "validator. "
             f"Default: {DEFAULT_VTRUST_THRESHOLD}")

    parser.add_argument(
        "--stopped-logs-threshold",
        type=float,
        default=DEFAULT_STOPPED_LOGS_THRESHOLD,
        help="The time in minutes after which to restart the process if the "
             "pm2 log files haven't updated. "
             f"Default: {DEFAULT_STOPPED_LOGS_THRESHOLD}")

    parser.add_argument(
        "--log-errors-restart-wait-time",
        type=float,
        default=DEFAULT_LOG_ERRORS_RESTART_WAIT_TIME,
        help="The number of minutes to wait after restarting a the validator "
             "due to a pm2 log patterns error so it doesn't get restarted multiple "
             "times due to duplicate or quickly recurring error patterns. NOTE: "
             "this only applies to pm2 logs. Docker logs are unaffected by this."
             f"Default: {DEFAULT_LOG_ERRORS_RESTART_WAIT_TIME}")

    parser.add_argument(
        "--code-repo-path",
        action="append",
        help="When specified, check this path for git code updates. When not specified, "
             "the current directory is checked. This arg can be specified multiple times "
             "to check multiple code repos.")

    parser.add_argument(
        "--do-vtrust-check",
        action="store_true",
        dest="do_check_vtrust",
        help="When specified, this will do the checking of the vTrust value. "
             "Note: Unlike most of the other checks, this one is opt-in "
             "rather than opt-out.")
    
    parser.add_argument(
        "--skip-code-check",
        action="store_false",
        dest="do_check_code",
        help="When specified, this will do the checking for git code updates. "
             "Note: Unlike most of the other checks, this one is opt-in "
             "rather than opt-out. WARNING: This check currentl requires that the "
             "restarter be run from the base git repo folder.")

    parser.add_argument(
        "--skip-updated-check",
        action="store_false",
        dest="do_check_updated",
        help="When specified, this will skip the checking of the Updated value.")

    parser.add_argument(
        "--skip-log-errors-check",
        action="store_false",
        dest="do_check_errors",
        help="When specified, this will skip the checking of the log output "
             "for error patterns.")

    parser.add_argument(
        "--skip-stopped-logs-check",
        action="store_false",
        dest="do_check_stopped_logs",
        help="When specified, this will skip the checking whether the pm2 log "
             "files sotpped udpdating.")

    parser.add_argument(
        "--skip-discord-notify",
        action="store_false",
        dest="discord_notify",
        help="When specified, this will skip sending the notification to the "
             "discord monitor channel.")

    return parser.parse_args()


error_logs_wait_timer = None


class MetagraphData:
    def __init__(self, metagraph):
        self.netuid = metagraph.netuid
        self.hotkeys = metagraph.hotkeys
        self.coldkeys = metagraph.coldkeys
        self.block = metagraph.block
        self.last_update = metagraph.last_update
        self.Tv = metagraph.Tv


def get_metagraph_data(network, netuid, mp_queue_name):
    mp_queue = globals()[mp_queue_name]
    with bittensor.Subtensor(network=network) as subtensor:
        metagraph = subtensor.metagraph(netuid=netuid)
        metagraph_data = MetagraphData(metagraph)
        mp_queue.put(metagraph_data)


class ValidatorChecker:
    # _rizzo_hotkey = "5FtBncJvGhxjBs4aFn2pid6aur9tBUuo9QR7sHe5DkoRizzo"
    _rizzo_coldkey = "5FuzgvtfbZWdKSRxyYVPAPYNaNnf9cMnpT7phL3s2T3Kkrzo"

    # This is a fix to handle the subnets on which we're registered on
    # multiple uids.
    _multi_uid_hotkeys = {
        20: "5ExaAP3ENz3bCJufTzWzs6J6dCWuhjjURT8AdZkQ5qA4As2o",
        86: "5F9FAMhhzZJBraryVEp1PTeaL5bgjRKcw1FSyuvRLmXBds86",
        123: "5GzaskJbqJvGGXtu2124i9YLgHfMDDr7Pduq6xfYYgkJs123",
        124: "5FKk6ucEKuKzLspVYSv9fVHonumxMJ33MdHqbVjZi2NUs124",
    }

    _local_subtensors = [
        "cali",
        "candyland",
        "datacenter01",
        "la",
        "moonbase",
        "titan",
    ]

    def __init__(self, options):
        self._netuid = options.netuid
        self._restarter = options.restarter_class(self, options)

        self._init_setup(options)

        self._run()

    def _get_rizzo_uid(self, metagraph_data):
        if metagraph_data.netuid in self._multi_uid_hotkeys:
            try:
                return metagraph_data.hotkeys.index(
                    self._multi_uid_hotkeys[metagraph_data.netuid]
                )
            except ValueError:
                return None

        try:
            return metagraph_data.coldkeys.index(self._rizzo_coldkey)
        except ValueError:
            return None

    # Updated and vTrust checkers only
    def _get_metagraph_data(self):
        # Loop until we get a subtensor connection
        while True:
            self._local_subtensor_index = \
                (self._local_subtensor_index + 1) % len(self._local_subtensors)
            network_name = self._local_subtensors[self._local_subtensor_index]
            network = f"ws://subtensor-{network_name}.rizzo.network:9944"

            self._log_info(f"Connecting to subtensor network: {network}")
            try:
                args = [network, self._netuid, self._mp_queue_name]
                with multiprocessing.Pool(processes=1) as pool:
                    pool.apply(get_metagraph_data, args)
                break
            except Exception as err:
                self._log_error("")
                self._log_error(f"Subtensor connection failed on '{network}'")
                self._log_error(f"{type(err).__name__}: {err}")
                self._log_error("")
                self._log_error("Rotating subtensors and trying again.")
                time.sleep(1)

        mp_queue = globals()[self._mp_queue_name]
        return mp_queue.get()

    @classmethod
    def _log_info(cls, message):
        bittensor.logging.info(f"{cls.log_prefix}: {message}")

    @classmethod
    def _log_error(cls, message):
        bittensor.logging.error(f"{cls.log_prefix}: {message}")

    @classmethod
    def _log_warning(cls, message):
        bittensor.logging.warning(f"{cls.log_prefix}: {message}")

    @classmethod
    def _log_debug(cls, message):
        bittensor.logging.debug(f"{cls.log_prefix}: {message}")

    def _init_setup(self, options):
        raise NotImplementedError("Must be implemented in subclasses.")

    def _run(self):
        raise NotImplementedError("Must be implemented in subclasses.")


class ValidatorCheckerUpdated(ValidatorChecker):
    log_prefix = "CHECK UPDATED"

    def _init_setup(self, options):
        self._restart_threshold = options.updated_threshold

        # Start false in case this is added after a manual restart
        # but before it started setting weights again
        self._check_for_restart = False

        # Randomize local subtensor
        random.seed()
        self._local_subtensor_index = random.randint(0, len(self._local_subtensors) - 1)

        # Create the multiprocessing queue for passing the metagraph data
        # from the subprocess back to the main process.
        global UPDATED_MP_QUEUE
        UPDATED_MP_QUEUE = multiprocessing.Queue()
        self._mp_queue_name = "UPDATED_MP_QUEUE"

    def _run(self):
        self._log_info("")
        self._log_info("Checking for high Updated values.")
        self._log_info("")

        default_sleep_time = 4320  # 360 blocks

        while True:
            metagraph_data = self._get_metagraph_data()
            rizzo_uid = self._get_rizzo_uid(metagraph_data)
            if rizzo_uid is None:
                self._log_warning(
                    f"Rizzo validator not running for subnet {self._netuid}. "
                )
                self._log_info(f"Sleeping for {default_sleep_time} seconds.")
                time.sleep(default_sleep_time)
                continue

            rizzo_updated = int(
                metagraph_data.block - metagraph_data.last_update[rizzo_uid])
            self._log_info("")
            self._log_info(f"Rizzo Updated is {rizzo_updated} blocks.")

            if self._check_for_restart:
                # If the rizzo updated value is greater than the restart threshold
                # the do a restart and set _check_for_restart to False.
                self._log_info("Updated value check for restart is True.")
                if rizzo_updated >= self._restart_threshold:
                    self._log_info(f"Updated value {rizzo_updated} "
                                   f">= {self._restart_threshold}")
                    self._restarter.do_restart(f"Updated value is {rizzo_updated}")
                    self._log_info("Setting check for restart to False.")
                    self._check_for_restart = False
                else:
                    self._log_info(f"Updated value {rizzo_updated} "
                                   f"< {self._restart_threshold}")
                    self._log_info("Doing nothing.")
            else:
                # If the rizzo updated value is less than the restart threshold
                # then set _check_for_restart to True.
                self._log_info("Updated value Check for restart is False.")
                if rizzo_updated < self._restart_threshold:
                    self._log_info(f"Updated value {rizzo_updated} "
                                   f"< {self._restart_threshold}")
                    self._log_info("Setting check for restart to True.")
                    self._check_for_restart = True
                else:
                    self._log_info(f"Updated value {rizzo_updated} "
                                   f">= {self._restart_threshold}")
                    self._log_info("Doing nothing.")

            seconds_until_threshold = \
                (self._restart_threshold - rizzo_updated) * 12
            sleep_interval = (seconds_until_threshold
                              if seconds_until_threshold > 0
                              else default_sleep_time)
            self._log_info(f"Sleeping for {sleep_interval} seconds.")
            time.sleep(sleep_interval)


class ValidatorCheckerVTrust(ValidatorChecker):
    log_prefix = "CHECK VTRUST"

    def _init_setup(self, options):
        self._restart_threshold = options.vtrust_threshold

        # Start false in case this is added after a manual restart
        # but before the vTrust goes back up again
        self._check_for_restart = False

        # Randomize local subtensor
        random.seed()
        self._local_subtensor_index = random.randint(0, len(self._local_subtensors) -1)

        # Create the multiprocessing queue for passing the metagraph data
        # from the subprocess back to the main process.
        global VTRUST_MP_QUEUE
        VTRUST_MP_QUEUE = multiprocessing.Queue()
        self._mp_queue_name = "VTRUST_MP_QUEUE"

    def _run(self):
        self._log_info("")
        self._log_info("Checking for low vTrust values.")
        self._log_info("")

        sleep_interval = 4320  # 360 blocks

        while True:
            metagraph_data = self._get_metagraph_data()
            rizzo_uid = self._get_rizzo_uid(metagraph_data)
            if rizzo_uid is None:
                self._log_warning(
                    f"Rizzo validator not running for subnet {self._netuid}. "
                )
                self._log_info(f"Sleeping for {sleep_interval} seconds.")
                time.sleep(sleep_interval)
                continue

            rizzo_vtrust = metagraph_data.Tv[rizzo_uid]
            vtrust_str = f"{rizzo_vtrust:.5f}"

            self._log_info("")
            self._log_info(f"Rizzo vTrust is {vtrust_str}")

            if self._check_for_restart:
                # If the rizzo vTrust value is less than the restart threshold
                # the do a restart and set _check_for_restart to False.
                self._log_info("vTrust value check for restart is True.")
                if rizzo_vtrust < self._restart_threshold:
                    self._log_info(f"vTrust value {vtrust_str} "
                                   f"< {self._restart_threshold}")
                    self._restarter.do_restart(f"vTrust value is {vtrust_str}")
                    self._log_info("Setting check for restart to False.")
                    self._check_for_restart = False
                else:
                    self._log_info(f"vTrust value {vtrust_str} "
                                   f">= {self._restart_threshold}")
                    self._log_info("Doing nothing.")
            else:
                # If the rizzo vTrust value is greater than the restart threshold
                # then set _check_for_restart to True.
                self._log_info("vTrust value Check for restart is False.")
                if rizzo_vtrust >= self._restart_threshold:
                    self._log_info(f"vTrust value {vtrust_str} "
                                   f">= {self._restart_threshold}")
                    self._log_info("Setting check for restart to True.")
                    self._check_for_restart = True
                else:
                    self._log_info(f"vTrust value {vtrust_str} "
                                   f"< {self._restart_threshold}")
                    self._log_info("Doing nothing.")

            self._log_info(f"Sleeping for {sleep_interval} seconds.")
            time.sleep(sleep_interval)


class ValidatorCheckerDockerLogErrors(ValidatorChecker):
    log_prefix = "CHECK DOCKER LOG ERRORS"
    _generic_patterns = [r"\[Errno 32\] Broken pipe"]
    _subnet_patterns = []

    def _init_setup(self, options):
        self._docker_container = options.docker_container

    def _run(self):
        self._log_info("")
        self._log_info("Checking for log patterns.")
        self._log_info("")

        if not self._generic_patterns and not self._subnet_patterns:
            self._log_warning(
                f"No log patterns defined for subnet {self._netuid}. "
                "Not checking for log patterns."
            )
            return

        log_regexes = []
        for log_pattern in self._generic_patterns + self._subnet_patterns:
            log_regexes.append(re.compile(log_pattern))
        command = ["docker", "logs", self._docker_container, "--since", "15s", "--follow"]
        command_str = " ".join(command)

        while True:
            self._log_info(f"Launching process: \"{command_str}\"")

            mfd, sfd = pty.openpty()
            process = subprocess.Popen(
                command, stdout=sfd, stderr=subprocess.STDOUT)
            os.close(sfd)
            master = os.fdopen(mfd)
            while True:
                try:
                    line = master.readline()
                except:
                    # The process exited.
                    self._log_info(f"Process exited: \"{command_str}\"")
                    break
                else:
                    do_restart = False
                    for log_regex in log_regexes:
                        match = log_regex.search(line)
                        if match:
                            do_restart = True
                            pattern = match.group()
                            self._log_info(
                                f"Log line matches a restart pattern: \"{pattern}\"\n"
                                f"{line}\n")
                            self._restarter.do_restart(
                                f"Docker log output matches a restart pattern: \"{pattern}\""
                            )
                            break
                    if do_restart:
                        break

            self._log_info(f"Killing process: \"{command_str}\"")
            process.kill()
            master.close()
            sleep_time = 15
            self._log_info(f"Sleeping {sleep_time} seconds")
            time.sleep(sleep_time)


class ValidatorCheckerDockerLogErrorsSn52(ValidatorCheckerDockerLogErrors):
    _generic_patterns = []
    _subnet_patterns = [r"websockets\.exceptions\.InvalidStatus"]

class ValidatorCheckerDockerLogErrorsSn59(ValidatorCheckerDockerLogErrors):
    _subnet_patterns = [r"\[websockets\.client\] unexpected internal error"]

class ValidatorCheckerDockerLogErrorsSn64(ValidatorCheckerDockerLogErrors):
    _generic_patterns = []
    _subnet_patterns = []  # [r"Weights set successfully\."]


class ValidatorCheckerPm2LogErrors(ValidatorChecker):
    log_prefix = "CHECK PM2 LOG ERRORS"
    _generic_patterns = [r"\[Errno 32\] Broken pipe"]
    _subnet_patterns = []
    _skip_initial_log_lines = 40

    # Inline wait timer class
    class ErrorLogsWaitTimer:
        def __init__(self, wait_time):
            self._timer_lock = threading.Lock()
            self._wait_timer = None
            self._wait_event = threading.Event()
            self._wait_time = wait_time

        def get_waiting_status(self):
            return self._wait_event.is_set()

        def start_wait_timer(self):
            with self._timer_lock:
                self._wait_event.set()
                if self._wait_timer:
                    self._wait_timer.cancel()
                self._wait_timer = threading.Timer(
                    interval=self._wait_time, function=self._unset_wait_event
                )
                self._wait_timer.start()
                ValidatorCheckerPm2LogErrors._log_info(
                    f"Stopping pm2 log patterns check. Waiting {self._wait_time} "
                    "seconds after restart to continue checking log patterns."
                )

        def _unset_wait_event(self):
            with self._timer_lock:
                self._wait_event.clear()
                ValidatorCheckerPm2LogErrors._log_info(
                    "Continuing pm2 log patterns check."
                )

    def _init_setup(self, options):
        self._pm2_process = options.pm2_process
        self._restart_wait_time = options.log_errors_restart_wait_time * 60

    def _run(self):
        self._log_info("")
        self._log_info("Checking for log patterns.")
        self._log_info("")

        if not self._generic_patterns and not self._subnet_patterns:
            self._log_warning(
                f"No log patterns defined for subnet {self._netuid}. "
                "Not checking for log patterns."
            )
            return

        self._create_error_logs_wait_timer(self._restart_wait_time)

        log_regexes = []
        for log_pattern in self._generic_patterns + self._subnet_patterns:
            log_regexes.append(re.compile(log_pattern))
        command = ["pm2", "log", self._pm2_process, "--raw"]
        command_str = " ".join(command)

        while True:
            _initial_log_lines = 0
            self._log_info(f"Launching process: \"{command_str}\"")

            mfd, sfd = pty.openpty()
            process = subprocess.Popen(
                command, stdout=sfd, stderr=subprocess.STDOUT)
            os.close(sfd)
            master = os.fdopen(mfd)
            while True:
                try:
                    line = master.readline()
                except:
                    # The process exited.
                    self._log_info(f"Process exited: \"{command_str}\"")
                    break
                else:
                    if _initial_log_lines < self._skip_initial_log_lines:
                        _initial_log_lines += 1
                        self._log_debug(f"{_initial_log_lines=}")
                        continue
                    elif _initial_log_lines == self._skip_initial_log_lines:
                        _initial_log_lines += 1
                        self._log_info("Starting log patterns check.")

                    if error_logs_wait_timer.get_waiting_status():
                        self._log_debug(
                            f"({self._pm2_process}) Log line skipped. "
                            "In waiting mode."
                        )
                        continue

                    for log_regex in log_regexes:
                        match = log_regex.search(line)
                        if match:
                            pattern = match.group()
                            self._log_info(
                                f"Log line matches a restart pattern: \"{pattern}\"\n"
                                f"{line}\n")
                            self._restarter.do_restart(
                                f"Pm2 log output matches a restart pattern: \"{pattern}\""
                            )
                            break

            self._log_info(f"Killing process: \"{command_str}\"")
            process.kill()
            master.close()
            sleep_time = 15
            self._log_info(f"Sleeping {sleep_time} seconds")
            time.sleep(sleep_time)

    @classmethod
    def _create_error_logs_wait_timer(cls, wait_time):
        global error_logs_wait_timer

        if not error_logs_wait_timer:
            error_logs_wait_timer = (
                cls.ErrorLogsWaitTimer(wait_time)
            )

class ValidatorCheckerPm2LogErrorsSn1(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"json\.decoder\.JSONDecodeError:"]

# class ValidatorCheckerPm2LogErrorsSn2(ValidatorCheckerPm2LogErrors):
#     _subnet_patterns = [r"Successfully checked out the latest release"]

class ValidatorCheckerPm2LogErrorsSn4(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn10(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"Error during validation"]

class ValidatorCheckerPm2LogErrorsSn16(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"ConnectionRefusedError"]

class ValidatorCheckerPm2LogErrorsSn18(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"asyncio\.exceptions\.CancelledError"]

# class ValidatorCheckerPm2LogErrorsSn20(ValidatorCheckerPm2LogErrors):
#     _subnet_patterns = [r"Error in forward: Failed to get task after \d+ attempts"]

class ValidatorCheckerPm2LogErrorsSn21(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"asyncio\.exceptions\.CancelledError"]

class ValidatorCheckerPm2LogErrorsSn24(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []
    # _subnet_patterns = [r"EOF occurred in violation of protocol"]

class ValidatorCheckerPm2LogErrorsSn27(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn28(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"Transaction has an ancient birth block"]

class ValidatorCheckerPm2LogErrorsSn29(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn30(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []
    # _subnet_patterns = [r"asyncio\.exceptions\.CancelledError"]

class ValidatorCheckerPm2LogErrorsSn34(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [
        r"Handshake status 502 Bad Gateway",
        r"Error during validation",
    ]

class ValidatorCheckerPm2LogErrorsSn36(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn38(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"\[Errno -2\] Name or service not known"]

class ValidatorCheckerPm2LogErrorsSn41(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn42(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"EOF occurred in violation of protocol"]

class ValidatorCheckerPm2LogErrorsSn43(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"Error during validation"]

class ValidatorCheckerPm2LogErrorsSn46(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn52(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []
    _subnet_patterns = [r"websockets\.exceptions\.InvalidStatus"]

class ValidatorCheckerPm2LogErrorsSn55(ValidatorCheckerPm2LogErrors):
    _subnet_patterns = [r"websockets\.exceptions\.InvalidStatus"]

class ValidatorCheckerPm2LogErrorsSn79(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

class ValidatorCheckerPm2LogErrorsSn83(ValidatorCheckerPm2LogErrors):
    _generic_patterns = []

# class ValidatorCheckerPm2LogErrorsSn91(ValidatorCheckerPm2LogErrors):
#     _subnet_patterns = [r"Successfully set weights"]


class ValidatorCheckerPm2StoppedLogs(ValidatorChecker):
    log_prefix = "CHECK PM2 LOGS STOPPED"

    def _init_setup(self, options):
        self._pm2_process = options.pm2_process
        self._restart_threshold = int(round(options.stopped_logs_threshold * 60))

    def _run(self):
        self._log_info("")
        self._log_info("Checking for stopped logs.")

        while True:
            process = subprocess.run(["pm2", "jlist"], stdout=subprocess.PIPE)
            pm2_output = json.loads(process.stdout)
            for pm2_process in pm2_output:
                if pm2_process["name"] == self._pm2_process:
                    out_log_file =  pm2_process["pm2_env"]["pm_out_log_path"]
                    error_log_file =  pm2_process["pm2_env"]["pm_err_log_path"]
                    break

            self._log_info("")
            self._log_info(f"Out Log file: {out_log_file}")
            self._log_info(f"Error Log file: {error_log_file}")

            out_log_file_mtime = int(os.path.getmtime(out_log_file))
            error_log_file_mtime = int(os.path.getmtime(error_log_file))
            current_time = int(time.time())
            out_log_file_ctime = time.ctime(out_log_file_mtime)
            error_log_file_ctime = time.ctime(error_log_file_mtime)
            current_ctime = time.ctime(current_time)
            self._log_info("")
            self._log_info(f"Out Log file last modified: {out_log_file_ctime}")
            self._log_info(f"Error Log file last modified: {error_log_file_ctime}")
            self._log_info(f"Current time: {current_ctime}")

            time_diff = current_time - max(out_log_file_mtime, error_log_file_mtime)
            if time_diff >= self._restart_threshold:
                self._log_info(f"Time difference {time_diff} seconds "
                               f">= {self._restart_threshold} seconds")
                log_minutes = time_diff / 60
                self._restarter.do_restart(f"No log output in {log_minutes:.1f} minutes.")
            else:
                self._log_info(f"Time difference {time_diff} seconds "
                               f"< {self._restart_threshold} seconds")
                self._log_info("Doing nothing.")

            seconds_until_threshold = \
                (self._restart_threshold - time_diff)
            sleep_interval = (seconds_until_threshold
                              if seconds_until_threshold > 0
                              else self._restart_threshold)
            self._log_info(f"Sleeping for {sleep_interval} seconds.")
            time.sleep(sleep_interval)


class ValidatorCheckerCodeUpdate(ValidatorChecker):
    log_prefix = "CHECK CODE UPDATE"

    def _init_setup(self, options):
        if options.code_repo_path:
            # Get the input repo paths
            self._code_repo_paths = [
                os.path.expanduser(p) for p in options.code_repo_path
            ]
        else:
            # Get the repo path from the current working directory
            git_cmd = "git rev-parse --show-toplevel"
            try:
                process = subprocess.run(
                    shlex.split(git_cmd), check=True, stdout=subprocess.PIPE
                )
            except subprocess.CalledProcessError:
                self._log_warning(
                    f"'{os.getcwd()}' is not a git repository."
                )
                # Try to get the code repo path from the validator pm2 processes
                # if they exist.
                self._code_repo_paths = self._get_repo_paths_from_pm2(options)
                if not self._code_repo_paths:
                    # Maybe this is running docker containers. Try to get the
                    # code repo path from the rsn.sh script.
                    self._code_repo_paths = \
                        self._get_repo_paths_from_restart_script(options)
            else:
                self._code_repo_paths = [process.stdout.decode().strip()]

    def _get_repo_paths_from_pm2(self, options):
        # Try to get the code repo path from the validator pm2 processes
        # if they exist.
        if not options.pm2_processes:
            return []

        repo_paths = set()

        process = subprocess.run(["pm2", "jlist"], stdout=subprocess.PIPE)
        pm2_output = json.loads(process.stdout)

        for pm2_process in pm2_output:
            pm2_name = pm2_process["name"]
            if pm2_name in options.pm2_processes:
                pm2_cwd = pm2_process["pm2_env"]["pm_cwd"]
                git_cmd = f"git -C {pm2_cwd} rev-parse --show-toplevel"
                try:
                    process = subprocess.run(
                        shlex.split(git_cmd), check=True, stdout=subprocess.PIPE
                    )
                except subprocess.CalledProcessError:
                    continue
                repo_path = process.stdout.decode().strip()
                self._log_info(
                    f"Found repo path from '{pm2_name}' pm2 process: {repo_path}"
                )
                repo_paths.add(repo_path)

        return list(repo_paths)

    def _get_repo_paths_from_restart_script(self, options):
        # Try to get the code repo path from the rsn.sh script.
        cd_regex = re.compile(r"^\s*cd\s+(?P<dir>.+?)\s+")
        repo_paths = set()
        cd_dir = ""

        restart_script = os.path.expanduser(options.restart_script)
        with open(restart_script, "r") as fp:
            script_lines = fp.readlines()

        for line in script_lines:
            regex_match = cd_regex.match(line)
            if not regex_match:
                continue
            cd_dir = os.path.join(
                cd_dir, os.path.expanduser(regex_match.group("dir"))
            )
            git_cmd = f"git -C {cd_dir} rev-parse --show-toplevel"
            try:
                process = subprocess.run(
                    shlex.split(git_cmd), check=True, stdout=subprocess.PIPE
                )
            except subprocess.CalledProcessError:
                continue
            repo_path = process.stdout.decode().strip()
            self._log_info(
                f"Found repo path from {restart_script} script: {repo_path}"
            )
            repo_paths.add(repo_path)

        return list(repo_paths)

    def _run(self):
        self._log_info("")
        self._log_info("Checking for code updates.")
        self._log_info("")
        
        if not self._code_repo_paths:
            self._log_error(
                "No valid git repo path. Not checking for code updates."
            )
            return

        while True:
            do_restart = False
            for code_repo_path in self._code_repo_paths:
                self._log_info("")
                self._log_info(f"Checking repo path: {code_repo_path}")
                self._log_info("")
                if not os.path.isdir(code_repo_path):
                    self._log_error(
                        f"Repo directory path does not exist: {code_repo_path}"
                    )
                    continue

                git_command = (
                    f"git -C {code_repo_path}"
                    if code_repo_path != os.getcwd()
                    else "git"
                )
                do_restart |= self._check_code_repo(git_command)

            if do_restart:
                self._restarter.do_restart(
                    "Pulled new code from git repo.",
                    force_notify=True
                )

            sleep_interval = 900  # 15 minutes
            self._log_info(f"Sleeping for {sleep_interval} seconds.")
            time.sleep(sleep_interval)

    def _check_code_repo(self, git_command):
        get_cmd = f"{git_command} rev-parse HEAD"
        pull_cmd = f"{git_command} pull --autostash"

        # Get commit then pull then get commit again. First get commit is in case
        # the code was manually pulled while we were waiting. This ensures that
        # we are always comparing the correct commits.
        try:
            process = subprocess.run(
                shlex.split(get_cmd), check=True, stdout=subprocess.PIPE
            )
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{get_cmd}' command failed with error: {exc}"
            )
            return False

        current_commit = process.stdout.decode().strip()

        try:
            subprocess.run(shlex.split(pull_cmd), check=True)
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{pull_cmd}' command failed with error: {exc}"
            )
            return False

        try:
            process = subprocess.run(
                shlex.split(get_cmd), check=True, stdout=subprocess.PIPE
            )
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{get_cmd}' command failed with error: {exc}"
            )
            return False

        new_commit = process.stdout.decode().strip()
        self._log_info("")
        self._log_info(f"Current commit: {current_commit}")
        self._log_info(f"New commit: {new_commit}")
        if current_commit != new_commit:
            self._log_info("Commits changed.")
            return True

        self._log_info("Commits are the same. Doing nothing.")
        return False


class ValidatorCheckerCodeUpdateLatestTag(ValidatorCheckerCodeUpdate):

    def _check_code_repo(self, git_command):
        fetch_cmd = f"{git_command} fetch"
        get_cmd = f"{git_command} describe --tags"
        current_cmd = f"{git_command} rev-parse HEAD"
        latest_cmd = f"{git_command} rev-list --tags --max-count=1"
        pull_cmd = f"{git_command} checkout"

        try:
            subprocess.run(shlex.split(fetch_cmd), check=True)
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{fetch_cmd}' command failed with error: {exc}"
            )
            return False

        try:
            process = subprocess.run(
                shlex.split(current_cmd), check=True, stdout=subprocess.PIPE
            )
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{current_cmd}' command failed with error: {exc}"
            )
            return False

        current_rev = process.stdout.decode().strip()

        try:
            get_current_cmd = f"{get_cmd} {current_rev}"
            process = subprocess.run(
                shlex.split(get_current_cmd), check=True, stdout=subprocess.PIPE
            )
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{get_current_cmd}' command failed with error: {exc}"
            )
            return False

        current_tag = process.stdout.decode().strip()

        try:
            process = subprocess.run(
                shlex.split(latest_cmd), check=True, stdout=subprocess.PIPE
            )
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{latest_cmd}' command failed with error: {exc}"
            )
            return False

        latest_rev = process.stdout.decode().strip()

        try:
            get_latest_cmd = f"{get_cmd} {latest_rev}"
            process = subprocess.run(
                shlex.split(get_latest_cmd), check=True, stdout=subprocess.PIPE
            )
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{get_latest_cmd}' command failed with error: {exc}"
            )
            return False

        latest_tag = process.stdout.decode().strip()

        self._log_info("")
        self._log_info(f"Current tag: {current_tag}")
        self._log_info(f"Latest tag: {latest_tag}")
        if latest_tag.endswith("-rc"):
            self._log_info("Latest tag is not a release. Doing nothing.")
            return False

        if current_tag != latest_tag:
            self._log_info("Tags are different.")
            try:
                pull_latest_cmd = f"{pull_cmd} {latest_tag}"
                subprocess.run(shlex.split(pull_latest_cmd), check=True)
            except subprocess.CalledProcessError as exc:
                self._log_error(
                    f"'{pull_latest_cmd}' command failed with error: {exc}"
                )
                return False

            self._log_info("Pulled latest tag.")
            return True

        self._log_info("Tags are the same. Doing nothing.")
        return False


class ValidatorRestarterRsnScript:
    # Old one on Rizzo server
    #
    # _discord_monitor_url = (
        # "https://discord.com/api/webhooks/1307044814348488704/"
        # "uYEDTqa89CI-NPau5zDyET_JTGt1r2j1u3ARvSEVCKP7-nnOfvCoUoEKwdZCLoiXKsjV"
    # )
    #
    # New one on VO-R server
    #
    _discord_monitor_url = (
        "https://discord.com/api/webhooks/1328849265765777468/"
        "yJg07DYWLJyiFZgZPaLGTmFEwiAu2JWW5osyjFVoqlMWT66JBbV9_FOcslvDdtibtcR0"
    )

    def __init__(self, checker_obj, options):
        self._checker_obj = checker_obj
        self._netuid = options.netuid
        self._discord_notify = options.discord_notify
        self._restart_script =  os.path.expanduser(options.restart_script)
        self._restart_venv = (
            os.path.expanduser(options.restart_venv)
            if options.restart_venv else None)

        self._restart_dir = os.path.expanduser(
            f"~/restart_scripts_sn{options.netuid}")
        os.makedirs(self._restart_dir, exist_ok=True)

        for restart_file in os.listdir(self._restart_dir):
            restart_file_path = os.path.join(self._restart_dir, restart_file)
            if os.path.isfile(restart_file_path):
                os.unlink(restart_file_path)

    def _log_info(self, message):
        bittensor.logging.info(f"{self._checker_obj.log_prefix}: {message}")

    def _log_error(self, message):
        bittensor.logging.error(f"{self._checker_obj.log_prefix}: {message}")

    def do_restart(self, description, force_notify=False):
        # TODO: acquire threading lock
        if error_logs_wait_timer:
            error_logs_wait_timer.start_wait_timer()
        self._do_restart(description, force_notify)

    def _do_restart(self, description, force_notify):
        script_name =  os.path.basename(__file__)

        self._log_info(f"Restarting subnet {self._netuid}: {description}.")
        self._log_info(f"Running script: {self._restart_script}")
        if self._restart_venv:
            self._log_info(f"Running in venv: {self._restart_venv}")

            fd, restart_script = tempfile.mkstemp(dir=self._restart_dir,
                prefix = f"restart_{self._netuid}_", suffix=".sh")
            os.close(fd)
            os.chmod(restart_script, 0o700)

            self._log_info(f"Packaging venv and script into {restart_script}")
            with open(restart_script, "w") as fd:
                venv_activate = os.path.join(self._restart_venv, "bin/activate")
                fd.write("#!/bin/bash\n"
                            "\n"
                            "deactivate\n"
                            f"source {venv_activate}\n"
                            f"{self._restart_script}\n")
            restart_cmd = [restart_script]
        else:
            restart_cmd = [self._restart_script]
            restart_script = None

        restart_cmd_str = " ".join(restart_cmd)
        self._log_info(f"Running command: '{restart_cmd_str}'")
        try:
            subprocess.run(restart_cmd, check=True)
        except subprocess.CalledProcessError as exc:
            self._log_error(
                f"'{restart_cmd_str}' command failed with error: {exc}")
            self._send_monitor_notification(
                f"{script_name}: Failed to restart subnet {self._netuid} - {description}",
                force_notify
            )
            return False

        if restart_script:
            os.unlink(restart_script)
        self._log_info(f"Subnet '{self._netuid}' successfully restarted.")
        self._send_monitor_notification(
            f"{script_name}: Successfully restarted on subnet {self._netuid} - {description}",
            force_notify
        )

        return True

    def _send_monitor_notification(self, message, force_notify):
        if not force_notify and not self._discord_notify:
            self._log_info("Not sending discord monitor notification.")
            return

        payload = json.dumps({"content": message})
        monitor_cmd = [
            "curl", "-H", "Content-Type: application/json",
            "-d", payload, self._discord_monitor_url
        ]
        monitor_cmd_str = " ".join(monitor_cmd)
        self._log_info(f"Running command: '{monitor_cmd_str}'")
        try:
            subprocess.run(monitor_cmd, check=True)
        except subprocess.CalledProcessError as exc:
            self._log_error("Failed to send discord monitor notification.")
            self._log_error(
                f"'{monitor_cmd_str}' command failed with error {exc}")
        else:
            self._log_info("Discord monitor notification successfully sent.")


PM2_LOGS_VALIDATORS_DICT = {
    1: ValidatorCheckerPm2LogErrorsSn1,
    # 2: ValidatorCheckerPm2LogErrorsSn2,
    4: ValidatorCheckerPm2LogErrorsSn4,
    10: ValidatorCheckerPm2LogErrorsSn10,
    16: ValidatorCheckerPm2LogErrorsSn16,
    18: ValidatorCheckerPm2LogErrorsSn18,
    21: ValidatorCheckerPm2LogErrorsSn21,
    # 20: ValidatorCheckerPm2LogErrorsSn20,
    24: ValidatorCheckerPm2LogErrorsSn24,
    27: ValidatorCheckerPm2LogErrorsSn27,
    28: ValidatorCheckerPm2LogErrorsSn28,
    29: ValidatorCheckerPm2LogErrorsSn29,
    30: ValidatorCheckerPm2LogErrorsSn30,
    34: ValidatorCheckerPm2LogErrorsSn34,
    36: ValidatorCheckerPm2LogErrorsSn36,
    38: ValidatorCheckerPm2LogErrorsSn38,
    41: ValidatorCheckerPm2LogErrorsSn41,
    42: ValidatorCheckerPm2LogErrorsSn42,
    43: ValidatorCheckerPm2LogErrorsSn43,
    46: ValidatorCheckerPm2LogErrorsSn46,
    52: ValidatorCheckerPm2LogErrorsSn52,
    55: ValidatorCheckerPm2LogErrorsSn55,
    79: ValidatorCheckerPm2LogErrorsSn79,
    83: ValidatorCheckerPm2LogErrorsSn83,
    # 91: ValidatorCheckerPm2LogErrorsSn91,
}


DOCKER_LOGS_VALIDATORS_DICT = {
    52: ValidatorCheckerDockerLogErrorsSn52,
    59: ValidatorCheckerDockerLogErrorsSn59,
    64: ValidatorCheckerDockerLogErrorsSn64,
}


GIT_TAG_VALIDATORS = [2, 50, 70]


def _run_checker(checker_class, options):
    checker_class(options)


def main(options):
    if DEBUG:
        bittensor.logging.enable_debug()
    else:
        bittensor.logging.enable_info()

    if (options.do_check_errors
        and not (options.pm2_processes or options.docker_containers)
    ):
        bittensor.logging.warning(
            "No --pm2-process or --docker-container is specified. "
            "Not checking for log errors.")
        options.do_check_errors = False

    if options.do_check_stopped_logs and not options.pm2_processes:
        bittensor.logging.warning(
            "No --pm2-process is specified. Not checking for "
            "stopped pm2 log output.")
        options.do_check_stopped_logs = False

    sleep_time = 15
    script_name =  os.path.basename(__file__)
    bittensor.logging.info("")
    bittensor.logging.info(f"Starting {script_name} on subnet {options.netuid}")
    bittensor.logging.info(f"Sleeping {sleep_time} seconds in case the "
                            "validator process is just starting.")
    bittensor.logging.info("")
    time.sleep(sleep_time)

    docker_log_checker_class = DOCKER_LOGS_VALIDATORS_DICT.get(
        options.netuid, ValidatorCheckerDockerLogErrors)
    pm2_log_checker_class = PM2_LOGS_VALIDATORS_DICT.get(
        options.netuid, ValidatorCheckerPm2LogErrors)

    code_checker_class = (
        ValidatorCheckerCodeUpdateLatestTag if options.netuid in GIT_TAG_VALIDATORS
        else ValidatorCheckerCodeUpdate
    )

    options.restarter_class = ValidatorRestarterRsnScript
    with ThreadPoolExecutor(max_workers=TOTAL_CHECKS) as executor:
        if options.do_check_updated:
            bittensor.logging.info("")
            bittensor.logging.info("===============================")
            bittensor.logging.info("Running Updated value checker.")
            bittensor.logging.info("===============================")
            bittensor.logging.info("")
            executor.submit(_run_checker, ValidatorCheckerUpdated, options)

            bittensor.logging.info(f"Sleeping {sleep_time} seconds before "
                                    "starting the next one.")
            time.sleep(sleep_time)

        if options.do_check_vtrust:
            bittensor.logging.info("")
            bittensor.logging.info("==============================")
            bittensor.logging.info("Running vTrust value checker.")
            bittensor.logging.info("==============================")
            bittensor.logging.info("")
            executor.submit(_run_checker, ValidatorCheckerVTrust, options)

            bittensor.logging.info(f"Sleeping {sleep_time} seconds before "
                                    "starting the next one.")
            time.sleep(sleep_time)

        if options.do_check_errors:
            for docker_container in options.docker_containers:
                options.docker_container = docker_container # Kinda dumb but oh well
                bittensor.logging.info("")
                bittensor.logging.info("===================================")
                bittensor.logging.info("Running docker log errors checker.")
                bittensor.logging.info("===================================")
                bittensor.logging.info("")
                executor.submit(_run_checker, docker_log_checker_class, options)

                bittensor.logging.info(f"Sleeping {sleep_time} seconds before "
                                        "starting the next one.")
                time.sleep(sleep_time)

            for pm2_process in options.pm2_processes:
                options.pm2_process = pm2_process # Kinda dumb but oh well
                bittensor.logging.info("")
                bittensor.logging.info("================================")
                bittensor.logging.info("Running pm2 log errors checker.")
                bittensor.logging.info("================================")
                bittensor.logging.info("")
                executor.submit(_run_checker, pm2_log_checker_class, options)

                bittensor.logging.info(f"Sleeping {sleep_time} seconds before "
                                        "starting the next one.")
                time.sleep(sleep_time)

        if options.do_check_stopped_logs:
            for pm2_process in options.pm2_processes:
                options.pm2_process = pm2_process # Kinda dumb but oh well
                bittensor.logging.info("")
                bittensor.logging.info("==================================")
                bittensor.logging.info("Running stopped pm2 logs checker.")
                bittensor.logging.info("==================================")
                bittensor.logging.info("")
                executor.submit(_run_checker, ValidatorCheckerPm2StoppedLogs, options)

                bittensor.logging.info(f"Sleeping {sleep_time} seconds before "
                                       "starting the next one.")
                time.sleep(sleep_time)
        
        if options.do_check_code:
            bittensor.logging.info("")
            bittensor.logging.info("=============================")
            bittensor.logging.info("Running code update checker.")
            bittensor.logging.info("=============================")
            bittensor.logging.info("")
            executor.submit(_run_checker, code_checker_class, options)


if __name__ == "__main__":
    options = parse_args()

    # Importing bittensor here suppresses the --help info
    import bittensor

    main(options)
